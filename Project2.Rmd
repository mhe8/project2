---
title: "Project 1:  predictive models"
author: 
- "Min He"
date: "July 2, 2020 (updated `r Sys.Date()`)"
output: rmarkdown::github_document
params:
  weekday: Monday
---
  
```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE)
library(readr)
library(tidyverse)
library(MASS)
library(caret)
library(randomForest)
```

# Project report for `r params$weekday`

## Introduction about the data

This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks (popularity). [source](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity). In this project, I will try to use the logistic regression and bagging to classify whether the shares exceeds 1400 or not.

It includes the following columns:

0. url: URL of the article (non-predictive)
1. timedelta: Days between the article publication and the dataset acquisition (non-predictive)
2. n_tokens_title: Number of words in the title
3. n_tokens_content: Number of words in the content
4. n_unique_tokens: Rate of unique words in the content
5. n_non_stop_words: Rate of non-stop words in the content
6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content
7. num_hrefs: Number of links
8. num_self_hrefs: Number of links to other articles published by Mashable
9. num_imgs: Number of images
10. num_videos: Number of videos
11. average_token_length: Average length of the words in the content
12. num_keywords: Number of keywords in the metadata
13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?
14. data_channel_is_entertainment: Is data channel 'Entertainment'?
15. data_channel_is_bus: Is data channel 'Business'?
16. data_channel_is_socmed: Is data channel 'Social Media'?
17. data_channel_is_tech: Is data channel 'Tech'?
18. data_channel_is_world: Is data channel 'World'?
19. kw_min_min: Worst keyword (min. shares)
20. kw_max_min: Worst keyword (max. shares)
21. kw_avg_min: Worst keyword (avg. shares)
22. kw_min_max: Best keyword (min. shares)
23. kw_max_max: Best keyword (max. shares)
24. kw_avg_max: Best keyword (avg. shares)
25. kw_min_avg: Avg. keyword (min. shares)
26. kw_max_avg: Avg. keyword (max. shares)
27. kw_avg_avg: Avg. keyword (avg. shares)
28. self_reference_min_shares: Min. shares of referenced articles in Mashable
29. self_reference_max_shares: Max. shares of referenced articles in Mashable
30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable
31. weekday_is_monday: Was the article published on a Monday?
32. weekday_is_tuesday: Was the article published on a Tuesday?
33. weekday_is_wednesday: Was the article published on a Wednesday?
34. weekday_is_thursday: Was the article published on a Thursday?
35. weekday_is_friday: Was the article published on a Friday?
36. weekday_is_saturday: Was the article published on a Saturday?
37. weekday_is_sunday: Was the article published on a Sunday?
38. is_weekend: Was the article published on the weekend?
39. LDA_00: Closeness to LDA topic 0
40. LDA_01: Closeness to LDA topic 1
41. LDA_02: Closeness to LDA topic 2
42. LDA_03: Closeness to LDA topic 3
43. LDA_04: Closeness to LDA topic 4
44. global_subjectivity: Text subjectivity
45. global_sentiment_polarity: Text sentiment polarity
46. global_rate_positive_words: Rate of positive words in the content
47. global_rate_negative_words: Rate of negative words in the content
48. rate_positive_words: Rate of positive words among non-neutral tokens
49. rate_negative_words: Rate of negative words among non-neutral tokens
50. avg_positive_polarity: Avg. polarity of positive words
51. min_positive_polarity: Min. polarity of positive words
52. max_positive_polarity: Max. polarity of positive words
53. avg_negative_polarity: Avg. polarity of negative words
54. min_negative_polarity: Min. polarity of negative words
55. max_negative_polarity: Max. polarity of negative words
56. title_subjectivity: Title subjectivity
57. title_sentiment_polarity: Title polarity
58. abs_title_subjectivity: Absolute subjectivity level
59. abs_title_sentiment_polarity: Absolute polarity level
60. shares: Number of shares in social media (this is the dependent variable)


## Read in the news popularity data

```{r}
setwd("/home/aaron/Documents/NCSU_MinHe/ST558/project2_question")
pop_raw <- read.csv("OnlineNewsPopularity.csv", header = TRUE)
```

## Data preprocessing

* Combine the weekday flags into one variable `news_day`, e.g.: when `weekday_is_monday`=1, set the `news_day`='Monday'
* Combine the topic channels into one variable `data_channel`, e.g.: when `data_channel_is_lifestyle`=1, set the `data_channel`='lifestyle'
* Dividing the shares into two groups (< 1400 and â‰¥ 1400)

### Variables

* Those non-predictive variables (url and timedelta) won't be included in the model. 
* As in each model, we only use a subset of data (e.g.: Monday only), the weekday_is_monday, weekday_is_tuesday ... will be combined
* Basically, the vairables can be classified into the following classes
  + number of tokens (in title, content, keywords, stops, non-stops)
  + number of media in the article ( images, videos)
  + words characteristics (polarity, subjectivity, polarity, negative, positive, sentiment)
  + reference characteristics (shares of referenced, href)

```{r}

# Combine the weekday variables into one and set it to factor.
pop_raw$news_day <- rep("Sunday", nrow(pop_raw))
pop_raw$news_day[pop_raw$weekday_is_monday==1] <- "Monday"
pop_raw$news_day[pop_raw$weekday_is_tuesday==1] <- "Tuesday"
pop_raw$news_day[pop_raw$weekday_is_wednesday==1] <- "Wednesday"
pop_raw$news_day[pop_raw$weekday_is_thursday==1] <- "Thursday"
pop_raw$news_day[pop_raw$weekday_is_friday==1] <- "Friday"
pop_raw$news_day[pop_raw$weekday_is_saturday==1] <- "Saturday"
pop_raw$news_day <- factor(pop_raw$news_day) 

# Combine the channel variables into one and set it to factor.
pop_raw$data_channel <- rep('', nrow(pop_raw))
pop_raw$data_channel[pop_raw$data_channel_is_lifestyle==1] <- "lifestyle"
pop_raw$data_channel[pop_raw$data_channel_is_entertainment==1] <- "entertainment"
pop_raw$data_channel[pop_raw$data_channel_is_bus==1] <- "business"
pop_raw$data_channel[pop_raw$data_channel_is_socmed==1] <- "socmed"
pop_raw$data_channel[pop_raw$data_channel_is_tech==1] <- "tech"
pop_raw$data_channel[pop_raw$data_channel_is_world==1] <- "world"
pop_raw$data_channel <- factor(pop_raw$data_channel) 

pop_raw$shares_flag <- as.factor(pop_raw$shares>=1400)
```

## Clearing data

* Remove the unused/non-predictive variables
* Remove outliers (e.g.: n_tokens_content<=0)
* Subset the records to the day defined by the argument

```{r}
# Remove the non predictable variables
pop_raw<-pop_raw %>% subset(select=-c(url, timedelta, is_weekend, weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday, weekday_is_saturday,weekday_is_sunday, data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world))

# filter out outliers
pop_raw_post <- pop_raw %>% filter( n_tokens_content > 0 & n_unique_tokens >= 0 & n_unique_tokens <=1.0 & n_non_stop_words>=0 & n_non_stop_words<=1.0 & n_non_stop_unique_tokens>=0 & n_non_stop_unique_tokens<=1.0)

pop_1_day <- pop_raw_post %>% filter(news_day == params$weekday) %>% subset(select=-c(news_day))
```




## Divide dataset into training v.s. testing datasets

* Set random seeds so that the results can be replicated
* Break the dataset into 70% for training v.s. 30% for testing

```{r}
# Sampling the dataset into 80% : training data and 20% : test data:
set.seed(100)
popNewsTrain <- sample(nrow(pop_1_day),as.integer(nrow(pop_1_day)*0.7))
train.news = pop_1_day[popNewsTrain,]
test.news = pop_1_day[-popNewsTrain,]
```
## Data summary and exploratory analysis

### Boxplot for some of the key variables to identify the outliers

```{r}
# summary of the raw pop data
par(mfrow=c(3,5))
kay_variable <- c('n_tokens_title','n_tokens_content','num_hrefs', 'num_self_hrefs','num_keywords','kw_avg_min','kw_avg_max','kw_max_avg','kw_avg_avg','self_reference_avg_sharess','LDA_00','global_rate_positive_words','global_rate_negative_words','avg_positive_polarity','avg_negative_polarity')
for (col in kay_variable){
  boxplot(train.news[,col], xlab=col) 
}

```

### Scatterplot of the shares v.s. some othe key variables to identify the linear relationship (although we are using logistic linear regression, but the scatterplot is still useful to identify the patterns)

```{r}
par(mfrow=c(3,5))
for (col in kay_variable){
  plot(train.news[,col], train.news[,'shares'],  main = paste(col, ' v.s. shares'), xlab = col, ylab = 'shares')  
}
```

### Histogram for key variables, it's a good way to check the distribution (whether it's normal distribution or not)

```{r}
par(mfrow=c(3,5))
for (col in c('shares',kay_variable)){
  hist(train.news[,col],  main = paste('Histogram for ', col ), xlab = col, ylab = 'share')  
}

```

## Train the predictive model by using logistic regression

* Use both direction (forwards and backwards) stepwise methods to pick up the predict variables
* AIC is used as the criterion to pick the best combination of predict variables
* AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model. (excerpted from [linked wiki](https://en.wikipedia.org/wiki/Akaike_information_criterion))

```{r}
train.news <- train.news %>% subset(select=-c(shares))
test.news <- test.news %>% subset(select=-c(shares))

# Fit the full model 
# So try logistic regression model
full.model <- glm(shares_flag ~ ., data = train.news, family = "binomial")

# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both", trace = FALSE)
```

## Predict the value for test dataset with the trained logistic model

* If the predicted response is larger than 0.5, then it's shares>=1400
* Calculate/print the confusion matrix

```{r}
predict_value <- predict(step.model,test.news,type = "response") 
glm_confusionMatrix <- confusionMatrix(data = as.factor(predict_value>0.5), reference = as.factor(test.news$shares_flag))
glm_confusionMatrix
```

## Train the predictive model by using random forest

* Set number of trees to be 200


```{r}
bagFit <- randomForest(shares_flag ~ ., data = train.news, mtry = ncol(train.news) - 1, ntree = 200, importance = TRUE)
```


## Predict the value for test dataset with the trained bagging model

* If the predicted response is larger than 0.5, then it's shares>=1400
* Calculate/print the confusion matrix

```{r}
bagPred <- predict(bagFit, newdata = dplyr::select(test.news, -shares_flag))
bag_confusionMatrix <- confusionMatrix(data = bagPred, reference = as.factor(test.news$shares_flag))
bag_confusionMatrix
```

## Conclusion

The bagging methods are using classification trees.

* Create a bootstrap sample (same size as actual sample)
* Train tree on this sample
* Repeat B = 1000 times
* The voting method will be applied to pick which class it will belong.

The results obtained from the bagging and logistic regression show similar accuracy, and it's about 63% of accuracy for Monday only data.

